#!/usr/bin/env python
import requests
import re
from urllib.parse import urljoin

# Prompt the user for the target URL
target_url = input("Enter the target URL: ")
target_links = []

# Function to extract links from a given URL
def extract_links_from(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        if response.status_code == 200:
            links = re.findall(r'href=["\'](https?://.*?)(?=["\'])', response.text)
            return links
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
    return []

def crawl(url):
    href_links = extract_links_from(url)
    for link in href_links:
        if "#" in link:
            link = link.split("#")[0]
        absolute_link = urljoin(url, link)
        if absolute_link not in target_links:
            target_links.append(absolute_link)
            print(absolute_link)
            crawl(absolute_link)

# Call the crawl function with the user-provided target URL
crawl(target_url)
