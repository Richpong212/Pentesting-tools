#!/usr/bin/env python
import requests
import re
from urllib.parse import urljoin

# Prompt the user for the target URL
target_url = input("Enter the target URL: ")
target_links = []

# Function to extract links from a given URL
def extract_links_from(url):
    response = requests.get(url)
    if response.status_code == 200:
        links = re.findall(r'href=["\'](https?://.*?)(?=["\'])', response.text)
        return links
    else:
        return []

def crawl(url):
    href_links = extract_links_from(url)
    for link in href_links:
        if "#" in link:
            link = link.split("#")[0]
        absolute_link = urljoin(url, link)
        if absolute_link not in target_links:
            target_links.append(absolute_link)
            print(absolute_link)
            crawl(absolute_link)

# Call the crawl function with the user-provided target URL
crawl(target_url)
